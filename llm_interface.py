# llm_interface.py
# Batch 3: Adapting get_final_response to use task_outcome_status

import os
import json
import dashscope
from dashscope.api_entities.dashscope_response import GenerationResponse # ç¡®ä¿å¯¼å…¥æ­£ç¡®
import traceback

# å¯¼å…¥ Prompt æ¨¡æ¿
from prompts import (
    PLANNER_PROMPT_TEMPLATE,
    GENERAL_CHAT_PROMPT_TEMPLATE,
    PERSONA_NOT_FOUND_TEMPLATE,
    MODERATION_PROMPT_TEMPLATE
)
# Import knowledge_base only if its constants are directly used here for string matching.
# If PlannerAgent determines the 'not_found' status, direct import might not be needed here.
# For now, keeping it as it was, but ideally, this dependency for 'not_found' detection
# should be removed from here.
try:
    import knowledge_base # For potential access to NOT_FOUND messages if string matching is still a fallback
except ImportError:
    print("è­¦å‘Š: llm_interface.py æ— æ³•å¯¼å…¥ knowledge_baseã€‚æŸäº›åŠŸèƒ½å¯èƒ½å—å½±å“ã€‚")
    knowledge_base = None # Define it as None to prevent AttributeError if not found

# --- API Key é…ç½® (No changes from Batch 1) ---
api_key = os.getenv("DASHSCOPE_API_KEY")
if not api_key:
    print("é”™è¯¯ï¼šç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æœªè®¾ç½®ï¼ç¨‹åºå¯èƒ½æ— æ³•æ­£å¸¸è°ƒç”¨LLMã€‚")

# --- LLM æ¨¡å‹é…ç½® (No changes from Batch 1) ---
LLM_CONFIG = {
    "planner": os.getenv("PLANNER_LLM_MODEL", "qwen-turbo"),
    "response_generator": os.getenv("RESPONSE_LLM_MODEL", "qwen-turbo"),
    "moderator": os.getenv("MODERATOR_LLM_MODEL", "qwen-turbo")
}
print(f"LLM é…ç½®åŠ è½½: Planner='{LLM_CONFIG['planner']}', ResponseGenerator='{LLM_CONFIG['response_generator']}', Moderator='{LLM_CONFIG['moderator']}'")


def _call_dashscope_llm(prompt: str, model_id: str, context_for_debug: str = "LLM Call") -> str | None:
    # (No changes to this function from Batch 1)
    if not api_key:
        print(f"é”™è¯¯ ({context_for_debug}): API Key æœªé…ç½®ã€‚")
        return None
    dashscope.api_key = api_key

    print(f"--- Debug ({context_for_debug}): å‘é€ Prompt ç»™æ¨¡å‹ '{model_id}' (é¢„è§ˆå‰500å­—ç¬¦) ---\n{prompt[:500]}...\n--- Prompt é¢„è§ˆç»“æŸ ---")
    try:
        response = dashscope.Generation.call(
            model=model_id,
            prompt=prompt,
            result_format='message'
        )

        if response.status_code == 200 and response.output and response.output.choices:
            content = response.output.choices[0].get('message', {}).get('content')
            if content is None:
                print(f"é”™è¯¯ ({context_for_debug}): LLM å“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ 'content'ã€‚æ¨¡å‹: {model_id}, å“åº”: {response}")
                return None
            print(f"--- Debug ({context_for_debug}): LLM åŸå§‹å“åº” ---\n{content}\n--- LLM åŸå§‹å“åº”ç»“æŸ ---")
            return content.strip()
        else:
            print(f"é”™è¯¯ ({context_for_debug}): DashScope API è°ƒç”¨å¤±è´¥ã€‚æ¨¡å‹: {model_id}, çŠ¶æ€ç : {response.status_code}, é”™è¯¯ä¿¡æ¯: {getattr(response, 'message', 'N/A')}, Code: {getattr(response, 'code', 'N/A')}, è¯·æ±‚ID: {getattr(response, 'request_id', 'N/A')}")
            return None
    except Exception as e:
        print(f"é”™è¯¯ ({context_for_debug}): è°ƒç”¨ DashScope API æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ã€‚æ¨¡å‹: {model_id}")
        traceback.print_exc()
        return None

def get_llm_decision(user_input: str, chat_history: str, tools_description: str, llm_model_id: str = None) -> dict:
    # (No changes to this function from Batch 1)
    model_to_use = llm_model_id or LLM_CONFIG["planner"]
    prompt = PLANNER_PROMPT_TEMPLATE.format(
        user_input=user_input,
        chat_history=chat_history if chat_history else "æ— å¯¹è¯å†å²ã€‚",
        available_tools_description=tools_description
    )
    decision_json_str = _call_dashscope_llm(prompt, model_to_use, "LLM Planner Decision")

    if decision_json_str:
        try:
            text_to_parse = decision_json_str.strip()
            if text_to_parse.startswith("```json"):
                text_to_parse = text_to_parse[len("```json"):-len("```")].strip()
            elif text_to_parse.startswith("`json"):
                 text_to_parse = text_to_parse[len("`json"):-len("`")].strip()
            elif text_to_parse.startswith("```"):
                 text_to_parse = text_to_parse[len("```"):-len("```")].strip()

            decision = json.loads(text_to_parse)
            if 'action_type' not in decision:
                raise ValueError("LLM å†³ç­– JSON ä¸­ç¼ºå°‘ 'action_type' å­—æ®µã€‚")
            print(f"LLM Planner å†³ç­–å·²è§£æ: {decision}")
            return decision
        except (json.JSONDecodeError, ValueError) as e:
            print(f"é”™è¯¯: è§£æ LLM Planner çš„å†³ç­– JSON å¤±è´¥ - {e}\nLLM Planner åŸå§‹è¾“å‡ºä¸º: {decision_json_str}")
            fallback_response_text = "å­¦å§åœ¨æ€è€ƒä¸‹ä¸€æ­¥åšä»€ä¹ˆçš„æ—¶å€™å¥½åƒå‡ºäº†ç‚¹å°å·®é”™ï¼Œä½ èƒ½æ¢ä¸ªæ–¹å¼é—®é—®å—ï¼Ÿ(é”™è¯¯å‚è€ƒ: Planner JSONè§£æå¤±è´¥)"
            return {"action_type": "RESPOND_DIRECTLY", "response_content": fallback_response_text}
    else:
        fallback_response_text = "å“å‘€ï¼Œå­¦å§çš„æ€è·¯å¥½åƒå¡ä½äº†ï¼Œä½ èƒ½æ¢ä¸ªé—®æ³•å—ï¼Ÿ(é”™è¯¯å‚è€ƒ: Planner LLMè°ƒç”¨å¤±è´¥)"
        return {"action_type": "RESPOND_DIRECTLY", "response_content": fallback_response_text}


# MODIFICATION: Added task_outcome_status parameter
def get_final_response(user_input: str, context_info: str = "", llm_model_id: str = None, task_outcome_status: str | None = None) -> str:
    """
    è°ƒç”¨ Response Generator LLM è·å–æœ€ç»ˆçš„ã€é¢å‘ç”¨æˆ·çš„å›å¤ã€‚
    task_outcome_status: ç”± PlannerAgent æ ¹æ®å·¥å…·æ‰§è¡Œç»“æœç¡®å®šçš„æ€»ä½“ä»»åŠ¡çŠ¶æ€ (ä¾‹å¦‚ "success", "not_found", "failure").
    """
    model_to_use = llm_model_id or LLM_CONFIG["response_generator"]
    use_not_found_template = False

    # MODIFICATION: Prioritize task_outcome_status for deciding the template
    if task_outcome_status == "not_found":
        use_not_found_template = True
        print("--- Debug Final Response: task_outcome_status is 'not_found', using PERSONA_NOT_FOUND_TEMPLATE ---")
    elif task_outcome_status in ["failure", "error", "partial_failure", "no_steps_executed"] and not context_info:
        # If the task failed and there's no specific context, use a generic failure persona.
        # For simplicity, we can still use the not_found template or a new one.
        # Let's use not_found for now to indicate inability to fulfill the request.
        use_not_found_template = True # Or a new template for general failure
        print(f"--- Debug Final Response: task_outcome_status is '{task_outcome_status}' with no context, using PERSONA_NOT_FOUND_TEMPLATE as fallback ---")
    # Fallback to old string matching if task_outcome_status is not definitive and knowledge_base is available
    elif knowledge_base and isinstance(context_info, str) and not use_not_found_template:
        not_found_keywords = [
            knowledge_base.NOT_FOUND_STATIC_SLANG_MSG.split("å…³äº")[0].strip() if hasattr(knowledge_base, 'NOT_FOUND_STATIC_SLANG_MSG') else "###UNIQUE_NO_MATCH###",
            knowledge_base.NOT_FOUND_STATIC_FOOD_MSG.split("å“å‘€ï¼Œ")[1].split("å“¦")[0].strip() if hasattr(knowledge_base, 'NOT_FOUND_STATIC_FOOD_MSG') else "###UNIQUE_NO_MATCH###",
            knowledge_base.NOT_FOUND_ANY_INFO_FOR_QUERY_MSG.split("å…³äº")[1].split("ï¼Œ")[0].strip() if hasattr(knowledge_base, 'NOT_FOUND_ANY_INFO_FOR_QUERY_MSG') else "###UNIQUE_NO_MATCH###",
            "æš‚æ—¶æ²¡æœ‰æ‰¾åˆ°å’Œä½ é—®é¢˜ç›´æ¥ç›¸å…³çš„ä¿¡æ¯å‘¢", "å­¦å§æˆ‘å¥½åƒè¿˜æ²¡å­¦åˆ°å‘¢",
            "å°æœ¬æœ¬ä¸Šè¿˜æ²¡æœ‰å…³äº", "æš‚æ—¶æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆä½ è¦æ±‚çš„ç¾é£Ÿæ¨èä¿¡æ¯å“¦",
            "ç¿»äº†ç¿»å…±äº«çš„ç¬”è®°ï¼Œæš‚æ—¶æ²¡æœ‰æ‰¾åˆ°", "ä¸“å±å°æœ¬æœ¬é‡Œï¼Œå­¦å§æš‚æ—¶æ²¡æœ‰æ‰¾åˆ°",
        ]
        not_found_keywords = [kw for kw in not_found_keywords if kw and kw != "###UNIQUE_NO_MATCH###"]
        if any(keyword in context_info for keyword in not_found_keywords):
            use_not_found_template = True
            print("--- Debug Final Response: Detected 'æœªæ‰¾åˆ°' keywords in context_info, using PERSONA_NOT_FOUND_TEMPLATE ---")


    if use_not_found_template:
        prompt = PERSONA_NOT_FOUND_TEMPLATE.format(user_input=user_input)
    else:
        formatted_context = ""
        if context_info:
            formatted_context = f"\n\nèƒŒæ™¯å‚è€ƒä¿¡æ¯ï¼ˆè¯·ä¸è¦ç›´æ¥å¤è¿°ï¼Œè€Œæ˜¯è‡ªç„¶åœ°èå…¥ä½ çš„å›ç­”ä¸­ï¼‰ï¼š\n```\n{str(context_info)[:1000]}\n```\nè¯·ä½ ä½œä¸ºæ—¦æ—¦å­¦å§ï¼ŒåŸºäºä»¥ä¸ŠèƒŒæ™¯ä¿¡æ¯å’Œæˆ‘ä¹‹å‰çš„é—®é¢˜è¿›è¡Œå›å¤ã€‚"
        prompt = GENERAL_CHAT_PROMPT_TEMPLATE.format(user_input=user_input, context_info=formatted_context)

    reply_text = _call_dashscope_llm(prompt, model_to_use, "LLM Final Response")

    if reply_text:
        return reply_text
    else:
        # Provide a more specific error if the persona template was used due to failure
        if use_not_found_template and task_outcome_status and task_outcome_status != "not_found":
             return "å“å‘€ï¼Œå­¦å§åœ¨å°è¯•å›ç­”ä½ çš„æ—¶å€™é‡åˆ°äº†ä¸€ç‚¹å°éº»çƒ¦ï¼Œæ²¡èƒ½æ‰¾åˆ°æ»¡æ„çš„ç­”æ¡ˆï¼Œè¦ä¸æ¢ä¸ªé—®é¢˜è¯•è¯•ï¼ŸğŸ˜¥ (é”™è¯¯å‚è€ƒ: å›å¤ç”ŸæˆLLMè°ƒç”¨å¤±è´¥ï¼Œä»»åŠ¡å¤„ç†ä¸­é‡åˆ°é—®é¢˜)"
        return "å“å‘€ï¼Œå­¦å§åœ¨ç»„ç»‡è¯­è¨€çš„æ—¶å€™å¥½åƒå‡ºäº†ç‚¹å°å·®é”™ï¼Œä½ èƒ½å†è¯´ä¸€éå—ï¼ŸğŸ˜… (é”™è¯¯å‚è€ƒ: å›å¤ç”ŸæˆLLMè°ƒç”¨å¤±è´¥)"


def check_input_appropriateness(user_input: str, llm_model_id: str = None) -> dict:
    # (No changes to this function from Batch 1)
    model_to_use = llm_model_id or LLM_CONFIG["moderator"]
    prompt = MODERATION_PROMPT_TEMPLATE.format(user_input=user_input)

    moderation_json_str = _call_dashscope_llm(prompt, model_to_use, "Content Moderation")

    if moderation_json_str:
        try:
            text_to_parse = moderation_json_str.strip()
            if text_to_parse.startswith("```json"):
                text_to_parse = text_to_parse[len("```json"):-len("```")].strip()
            elif text_to_parse.startswith("`json"):
                 text_to_parse = text_to_parse[len("`json"):-len("`")].strip()
            elif text_to_parse.startswith("```"):
                 text_to_parse = text_to_parse[len("```"):-len("```")].strip()

            moderation_result = json.loads(text_to_parse)
            is_inappropriate = moderation_result.get("is_inappropriate", False)
            warning_message = moderation_result.get("warning_message")

            if not isinstance(is_inappropriate, bool):
                print(f"è­¦å‘Š: å®¡æ ¸LLMè¿”å›çš„ is_inappropriate ä¸æ˜¯å¸ƒå°”å€¼: {is_inappropriate}ã€‚å°†è§†ä¸ºä¸å½“å†…å®¹ã€‚")
                is_inappropriate = True
                warning_message = warning_message or "å­¦å§è§‰å¾—ä½ çš„è¯å¥½åƒæœ‰ç‚¹ä¸å¤ªåˆé€‚å“¦ï¼Œæˆ‘ä»¬æ¢ä¸ªè¯é¢˜å§ï¼"

            print(f"å†…å®¹å®¡æ ¸ç»“æœ: ä¸å½“å†…å®¹={is_inappropriate}, è­¦å‘Šæ¶ˆæ¯='{warning_message}'")
            return {"is_inappropriate": is_inappropriate, "warning_message": warning_message if is_inappropriate else None}
        except (json.JSONDecodeError, ValueError) as e:
            print(f"é”™è¯¯: è§£æå†…å®¹å®¡æ ¸LLMçš„JSONå“åº”å¤±è´¥ - {e}")
            print(f"å†…å®¹å®¡æ ¸LLMåŸå§‹è¾“å‡ºä¸º: {moderation_json_str}")
            return {"is_inappropriate": True, "warning_message": "ç³»ç»Ÿå®¡æ ¸æš‚æ—¶æœ‰ç‚¹å°è¿·ç³Šï¼Œä½†è¿˜æ˜¯è¯·æ³¨æ„å‹å¥½äº¤æµå“¦ï½ (é”™è¯¯å‚è€ƒ: å®¡æ ¸JSONè§£æå¤±è´¥)"}
    else:
        print("é”™è¯¯: å†…å®¹å®¡æ ¸LLMè°ƒç”¨å¤±è´¥ã€‚ä¸ºå®‰å…¨èµ·è§ï¼Œå°†é»˜è®¤æ ‡è®°ä¸ºä¸å½“ã€‚")
        return {"is_inappropriate": True, "warning_message": "ç³»ç»Ÿå®¡æ ¸æš‚æ—¶å‡ºå°å·®äº†ï¼Œä½†è¿˜æ˜¯è¯·æ³¨æ„å‹å¥½äº¤æµå“¦ï½ (é”™è¯¯å‚è€ƒ: å®¡æ ¸LLMè°ƒç”¨å¤±è´¥)"}



