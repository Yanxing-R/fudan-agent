# llm_interface.py
import os
import json
import dashscope
from dashscope.api_entities.dashscope_response import GenerationResponse
import traceback # å¼•å…¥ traceback

# å¯¼å…¥ Prompt æ¨¡æ¿ (ç¡®ä¿ prompts.py ä¸­æœ‰è¿™äº›å®šä¹‰)
from prompts import (
    PLANNER_PROMPT_TEMPLATE,
    GENERAL_CHAT_PROMPT_TEMPLATE,
    PERSONA_NOT_FOUND_TEMPLATE
)

# --- API Key é…ç½® ---
api_key = os.getenv("DASHSCOPE_API_KEY")
if not api_key:
    print("é”™è¯¯ï¼šç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æœªè®¾ç½®ï¼ç¨‹åºå¯èƒ½æ— æ³•æ­£å¸¸è°ƒç”¨LLMã€‚")
# dashscope.api_key = api_key # å»ºè®®åœ¨æ¯ä¸ªå‡½æ•°è°ƒç”¨å‰è®¾ç½®

# --- LLM æ¨¡å‹é…ç½® ---
LLM_CONFIG = {
    "planner": os.getenv("PLANNER_LLM_MODEL", "qwen-turbo"),
    "response_generator": os.getenv("RESPONSE_LLM_MODEL", "qwen-turbo"),
    # "knowledge_learner": os.getenv("LEARNER_LLM_MODEL", "qwen-turbo") # å¦‚æœéœ€è¦å•ç‹¬çš„learner
}
print(f"LLM é…ç½®åŠ è½½: Planner='{LLM_CONFIG['planner']}', ResponseGenerator='{LLM_CONFIG['response_generator']}'")


def _call_dashscope_llm(prompt: str, model_id: str, context_for_debug: str = "LLM Call") -> str | None:
    """å°è£…é€šç”¨çš„ DashScope LLM è°ƒç”¨é€»è¾‘"""
    if not api_key:
        print(f"é”™è¯¯ ({context_for_debug}): API Key æœªé…ç½®ã€‚")
        return None
    dashscope.api_key = api_key

    print(f"--- Debug ({context_for_debug}): å‘é€ Prompt ç»™æ¨¡å‹ '{model_id}' (é¢„è§ˆå‰500å­—ç¬¦) ---\n{prompt[:500]}...\n--- Prompt é¢„è§ˆç»“æŸ ---")
    try:
        response = dashscope.Generation.call(
            model=model_id,
            prompt=prompt,
            result_format='message'
        )
        if response.status_code == 200:
            content = response.output.choices[0]['message']['content']
            print(f"--- Debug ({context_for_debug}): LLM åŸå§‹å“åº” ---\n{content}\n--- LLM åŸå§‹å“åº”ç»“æŸ ---")
            return content.strip()
        else:
            print(f"é”™è¯¯ ({context_for_debug}): DashScope API è°ƒç”¨å¤±è´¥ã€‚æ¨¡å‹: {model_id}, çŠ¶æ€ç : {response.status_code}, é”™è¯¯ä¿¡æ¯: {getattr(response, 'message', 'N/A')}, è¯·æ±‚ID: {getattr(response, 'request_id', 'N/A')}")
            return None
    except Exception as e:
        print(f"é”™è¯¯ ({context_for_debug}): è°ƒç”¨ DashScope API æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ã€‚æ¨¡å‹: {model_id}")
        traceback.print_exc() # æ‰“å°å®Œæ•´é”™è¯¯å †æ ˆ
        return None


def get_llm_decision(user_input: str, chat_history: str, tools_description: str, llm_model_id: str = None) -> dict:
    """è°ƒç”¨ LLM (Planner) è¿›è¡Œä»»åŠ¡è§„åˆ’å’Œå·¥å…·é€‰æ‹©ã€‚è¿”å›ä¸€ä¸ªåŒ…å«å†³ç­–çš„å­—å…¸ã€‚"""
    model_to_use = llm_model_id or LLM_CONFIG["planner"]
    prompt = PLANNER_PROMPT_TEMPLATE.format(
        user_input=user_input,
        chat_history=chat_history if chat_history else "æ— å¯¹è¯å†å²ã€‚",
        available_tools_description=tools_description # è¿™é‡Œç°åœ¨æ˜¯ Specialist Agent çš„èƒ½åŠ›æè¿°
    )
    
    decision_json_str = _call_dashscope_llm(prompt, model_to_use, "LLM Planner Decision")

    if decision_json_str:
        try:
            text_to_parse = decision_json_str.strip()
            if text_to_parse.startswith("```json"):
                text_to_parse = text_to_parse[7:-3].strip()
            elif text_to_parse.startswith("`json"):
                 text_to_parse = text_to_parse[5:-1].strip()
            elif text_to_parse.startswith("```"):
                 text_to_parse = text_to_parse[3:-3].strip()

            decision = json.loads(text_to_parse)
            if 'action_type' not in decision:
                raise ValueError("LLM å†³ç­– JSON ä¸­ç¼ºå°‘ 'action_type' å­—æ®µã€‚")
            print(f"LLM Planner å†³ç­–å·²è§£æ: {decision}")
            return decision
        except (json.JSONDecodeError, ValueError) as e:
            print(f"é”™è¯¯: è§£æ LLM Planner çš„å†³ç­– JSON å¤±è´¥ - {e}")
            print(f"LLM Planner åŸå§‹è¾“å‡ºä¸º: {decision_json_str}")
            return {
                "action_type": "RESPOND_DIRECTLY", 
                "response_content": get_final_response(user_input, f"å­¦å§åœ¨æ€è€ƒä¸‹ä¸€æ­¥åšä»€ä¹ˆçš„æ—¶å€™å¥½åƒå‡ºäº†ç‚¹å°å·®é”™ ({e})ï¼Œä½ èƒ½æ¢ä¸ªæ–¹å¼é—®é—®å—ï¼Ÿ", llm_model_id=LLM_CONFIG["response_generator"])
            }
    else:
        return {
            "action_type": "RESPOND_DIRECTLY",
            "response_content": "å“å‘€ï¼Œå­¦å§çš„æ€è·¯å¥½åƒå¡ä½äº†ï¼Œä½ èƒ½æ¢ä¸ªé—®æ³•å—ï¼ŸğŸ¤”"
        }


def get_final_response(user_input: str, context_info: str = "", llm_model_id: str = None) -> str:
    """è°ƒç”¨ LLM (Response Generator) ç”Ÿæˆæœ€ç»ˆç»™ç”¨æˆ·çš„ã€å¸¦å­¦å§äººè®¾çš„å›å¤ã€‚"""
    model_to_use = llm_model_id or LLM_CONFIG["response_generator"]
    
    # æ ¹æ® context_info åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ PERSONA_NOT_FOUND_TEMPLATE
    # (è¿™ä¸ªé€»è¾‘å¯ä»¥æ›´ç²¾ç»†ï¼Œæ¯”å¦‚æ£€æŸ¥ context_info æ˜¯å¦åŒ…å«ç‰¹å®šçš„â€œæœªæ‰¾åˆ°â€å…³é”®è¯)
    use_not_found_template = False
    if isinstance(context_info, str):
        not_found_keywords = [
            "æŠ±æ­‰ï¼Œæˆ‘è¿˜ä¸çŸ¥é“", "å”‰å‘€ï¼Œæš‚æ—¶æ²¡æœ‰æ‰¾åˆ°", 
            "æš‚æ—¶æ²¡æœ‰æ‰¾åˆ°å’Œä½ é—®é¢˜ç›´æ¥ç›¸å…³çš„ä¿¡æ¯å‘¢", "å­¦å§æˆ‘å¥½åƒè¿˜æ²¡å­¦åˆ°å‘¢",
            "é™æ€çŸ¥è¯†åº“æš‚ä¸æ”¯æŒæŸ¥è¯¢", "çŸ¥è¯†Agentæœªèƒ½å¤„ç†ä»»åŠ¡" # æ¥è‡ª Specialist Agent çš„æ˜ç¡®å¤±è´¥ä¿¡å·
        ]
        if any(keyword in context_info for keyword in not_found_keywords):
            use_not_found_template = True
            print("--- Debug Final Response: æ£€æµ‹åˆ° 'æœªæ‰¾åˆ°' ä¸Šä¸‹æ–‡ï¼Œå°†ä½¿ç”¨ PERSONA_NOT_FOUND_TEMPLATE ---")

    if use_not_found_template:
        prompt = PERSONA_NOT_FOUND_TEMPLATE.format(user_input=user_input)
    else:
        formatted_context = ""
        if context_info:
            # æˆªæ–­è¿‡é•¿çš„ä¸Šä¸‹æ–‡ï¼Œé¿å…è¶…å‡ºLLMé™åˆ¶æˆ–å½±å“å›å¤é‡ç‚¹
            formatted_context = f"\n\nèƒŒæ™¯å‚è€ƒä¿¡æ¯ï¼ˆè¯·ä¸è¦ç›´æ¥å¤è¿°ï¼Œè€Œæ˜¯è‡ªç„¶åœ°èå…¥ä½ çš„å›ç­”ä¸­ï¼‰ï¼š\n```\n{str(context_info)[:1000]}\n```\nè¯·ä½ ä½œä¸ºæ—¦æ—¦å­¦å§ï¼ŒåŸºäºä»¥ä¸ŠèƒŒæ™¯ä¿¡æ¯å’Œæˆ‘ä¹‹å‰çš„é—®é¢˜è¿›è¡Œå›å¤ã€‚"
        
        prompt = GENERAL_CHAT_PROMPT_TEMPLATE.format(
            user_input=user_input,
            context_info=formatted_context
        )
    
    reply_text = _call_dashscope_llm(prompt, model_to_use, "LLM Final Response")

    if reply_text:
        return reply_text
    else:
        return "å“å‘€ï¼Œå­¦å§åœ¨ç»„ç»‡è¯­è¨€çš„æ—¶å€™å¥½åƒå‡ºäº†ç‚¹å°å·®é”™ï¼Œä½ èƒ½å†è¯´ä¸€éå—ï¼ŸğŸ˜…"

# structure_learned_knowledge å‡½æ•°å¯ä»¥ä¿æŒä¸å˜ï¼Œå¦‚æœ FudanKnowledgeAgent å†…éƒ¨éœ€è¦ç”¨ LLM è¾…åŠ©å­¦ä¹ 
